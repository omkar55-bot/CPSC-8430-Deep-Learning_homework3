================================================================================
                    SPOKEN-SQUAD QUESTION ANSWERING PROJECT
                        Deep Learning Homework Assignment 3
================================================================================

Student: [Your Name]
Date: October 24, 2025
Course: Deep Learning

================================================================================
TABLE OF CONTENTS
================================================================================
1. Introduction
2. Dataset Overview
3. Environment Setup
4. Model Architecture
5. Preprocessing Pipeline
6. Training Pipeline
7. Postprocessing Improvements
8. Experimental Configurations
9. Evaluation Results
10. Analysis and Observations
11. Challenges Faced
12. Conclusion

================================================================================
1. INTRODUCTION
================================================================================

This project implements a Question Answering (QA) system for the Spoken-SQuAD 
dataset, which contains questions and answers extracted from spoken language. 
The main goal is to build an extractive QA model that can accurately identify 
answer spans within given context passages.

The project explores four different training configurations (Simple, Medium, 
Strong, and Boss) to understand how various hyperparameters and techniques 
affect model performance. We used transformer-based models (BERT and ELECTRA) 
pretrained on Chinese text.

================================================================================
2. DATASET OVERVIEW
================================================================================

Dataset: Spoken-SQuAD (Chinese)
Format: SQuAD v1.1 JSON format

Dataset Splits:
- Training Set:   33,677 questions
- Validation Set:  3,434 questions  
- Test Set:        5,351 questions

Data Structure:
Each example contains:
- Question: A natural language question
- Context: A passage of text containing the answer
- Answer: The correct answer text (ground truth)
- Answer Start: Character position where answer begins in context

The dataset follows the nested SQuAD format with:
data -> articles -> paragraphs -> qa pairs

Key Characteristics:
- Questions are derived from spoken language (more natural, sometimes noisy)
- Contexts can be quite long (up to 512 tokens after truncation)
- Answers are typically short spans (5-30 characters)
- Some questions may have multiple valid answer spans

================================================================================
3. ENVIRONMENT SETUP
================================================================================

I set up a conda environment for this project to manage dependencies properly.

Step 1: Created environment.yml file
Specified all required packages:
- Python 3.9
- PyTorch 2.0.1 with CUDA 11.8 support
- Transformers 4.30.0 (for BERT/ELECTRA models)
- Accelerate 0.20.0 (for mixed precision training)
- Weights & Biases (for experiment tracking)
- Other utilities: tqdm, numpy, scikit-learn

Step 2: Created setup script (setup_conda.sh)
This automated the environment creation and activation.

Step 3: Installed the environment
Commands used:
```
chmod +x setup_conda.sh
./setup_conda.sh
conda activate dlhw3
```

Step 4: Verified installation
Checked that PyTorch could access GPU:
```
python -c "import torch; print(torch.cuda.is_available())"
```
Result: True (GPU available)

The environment setup was crucial because this project requires:
- GPU support for efficient training
- Specific library versions for compatibility
- Mixed precision training capabilities

================================================================================
4. MODEL ARCHITECTURE
================================================================================

I used transformer-based models from Hugging Face for this QA task.

Base Models Used:
1. BERT-base-chinese (Simple, Medium configs)
   - 12 transformer layers
   - 768 hidden dimensions
   - 12 attention heads
   - Pretrained on Chinese Wikipedia and other Chinese text

2. ELECTRA-base-chinese (Strong, Boss configs)
   - Model: hfl/chinese-electra-180g-base-discriminator
   - More efficient pretraining method than BERT
   - Better performance on downstream tasks
   - 12 layers, 768 hidden size

Architecture Components:
```
Input Layer:
- Tokenizer: BERT/ELECTRA WordPiece tokenizer
- Max sequence length: 512 tokens
- Special tokens: [CLS] question [SEP] context [SEP]

Transformer Encoder:
- Pretrained BERT/ELECTRA weights
- Self-attention mechanism
- Feed-forward networks
- Layer normalization
- Dropout for regularization

QA Output Head:
- Linear layer for start position prediction
- Linear layer for end position prediction
- Both produce logits over all token positions
```

The model predicts two things:
1. Start index: Where the answer begins
2. End index: Where the answer ends

During inference, we find the span with highest combined score:
score = start_logits[i] + end_logits[j]

================================================================================
5. PREPROCESSING PIPELINE
================================================================================

The preprocessing pipeline is crucial for preparing data for the QA model.

Step 1: Data Loading
- Load SQuAD format JSON files
- Flatten nested structure (data -> paragraphs -> qas)
- Create flat list of examples with question, context, answer

Step 2: Tokenization
- Use BERT/ELECTRA WordPiece tokenizer
- Tokenize question and context separately
- Combine with special tokens: [CLS] Q [SEP] C [SEP]
- Handle Chinese characters properly

Step 3: Sliding Window Approach (KEY IMPROVEMENT)
Why sliding windows?
- Some contexts are longer than 512 tokens
- We can't just truncate because answer might be cut off
- Solution: Create overlapping windows of the context

Implementation:
- max_length = 512 tokens
- doc_stride = 150 tokens (overlap between windows)
- Each window contains: full question + partial context

Example:
Context has 800 tokens:
- Window 1: tokens 0-350 (after question)
- Window 2: tokens 150-500 (overlap of 150 with window 1)
- Window 3: tokens 350-800 (overlap of 150 with window 2)

Step 4: Answer Position Mapping (PREPROCESSING IMPROVEMENT)
For training data, I needed to find where the answer is in each window.

Challenge: Answer might appear in multiple windows or none

Solution implemented:
- For each window, check if answer span falls within it
- Calculate token-level start and end positions
- If answer doesn't fit in window, mark as no-answer (position 0)

Improvement: Center answer in windows when possible
- During training, try to create windows where answer is centered
- This helps the model learn from better context
- Implementation in dataset.py: _create_training_windows()

Step 5: Padding and Attention Masks
- Pad sequences to max_length (512)
- Create attention mask: 1 for real tokens, 0 for padding
- Create token_type_ids: 0 for question, 1 for context

Step 6: Batch Collation
- Group examples into batches
- Convert to PyTorch tensors
- Move to GPU for training

Key preprocessing parameters:
- max_length: 512
- doc_stride: 150 (changed from default 128 for better coverage)
- padding: "max_length"
- truncation: True

================================================================================
6. TRAINING PIPELINE
================================================================================

The training pipeline was implemented with several advanced techniques.

Training Loop Structure:
```
For each epoch:
    For each batch:
        1. Forward pass through model
        2. Calculate loss (cross-entropy for start and end)
        3. Backward pass (compute gradients)
        4. Gradient accumulation (if used)
        5. Optimizer step
        6. Learning rate scheduling
    
    Evaluate on validation set
    Save checkpoint if best model
    Check early stopping
```

Key Components:

1. Loss Function
- Combined loss for start and end positions
- Cross-entropy loss for both:
  loss = loss_start + loss_end
- Model outputs: start_logits, end_logits
- Targets: start_position, end_position

2. Optimizer
- AdamW (Adam with weight decay)
- Better than regular Adam for transformers
- Weight decay applied to prevent overfitting
- Learning rates varied by configuration

3. Learning Rate Scheduling (IMPROVEMENT)
- Linear warmup + linear decay
- Warmup: gradually increase LR at start
- Helps with training stability
- Decay: gradually decrease LR to fine-tune

Implementation:
```
get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps = total_steps * warmup_ratio,
    num_training_steps = total_steps
)
```

4. Mixed Precision Training (FP16)
- Uses Accelerate library
- Trains with 16-bit floats instead of 32-bit
- Reduces memory usage by ~50%
- Speeds up training on modern GPUs
- Maintains accuracy with loss scaling

Important fix I made:
- Gradient clipping must only happen when gradients are synced
- Fixed bug where unscale_() was called twice
- Now: only clip when accelerator.sync_gradients = True

5. Gradient Accumulation
- Accumulate gradients over multiple batches
- Simulate larger batch sizes with limited GPU memory
- Effective batch size = batch_size Ã— accumulation_steps

6. Early Stopping
- Monitor validation F1 score
- Stop if no improvement for N epochs
- Prevents overfitting
- Saves training time

7. Checkpointing
- Save model every N epochs
- Save best model based on validation F1
- Save tokenizer and config
- For ensemble: save multiple models separately

8. Experiment Tracking
- Used Weights & Biases (wandb)
- Track metrics: loss, F1, exact match
- Log hyperparameters
- Compare different runs
- Create visualization dashboards

Gradient Clipping:
- Max gradient norm to prevent exploding gradients
- Applied after accumulation, before optimizer step
- Values: 0.5 to 1.0 depending on configuration

================================================================================
7. POSTPROCESSING IMPROVEMENTS
================================================================================

Postprocessing is critical for extracting the final answer from model outputs.

Challenge:
- Model outputs logits for every token position
- Need to find the best answer span
- Handle multiple windows for the same question
- Deal with edge cases

My Implementation:

Step 1: Collect Predictions from All Windows
- Each question may have multiple windows
- Each window produces start_logits and end_logits
- Group predictions by question ID

Step 2: Find Best Answer in Each Window
Algorithm:
```
For each window:
    Get top 20 start positions (highest logits)
    Get top 20 end positions (highest logits)
    
    For each start position:
        For each end position:
            # IMPROVEMENT: Validate span
            if end < start:
                skip  # Invalid span
            
            if end - start + 1 > max_answer_length:
                skip  # Answer too long
            
            score = start_logit + end_logit
            
            if score > best_score:
                Extract answer text
                Update best answer
```

Key Improvement (POSTPROCESSING FIX):
- Added check: if end_idx < start_idx, skip this span
- This prevents extracting invalid spans
- Previously, model sometimes predicted end before start
- This caused weird predictions with question text

Step 3: Select Best Answer Across Windows
- Compare scores from all windows
- Choose the answer with highest combined score
- This handles long contexts split across windows

Step 4: Answer Extraction
- Use tokenizer.decode() to convert token IDs to text
- skip_special_tokens=True (remove [CLS], [SEP])
- clean_up_tokenization_spaces=True
- Strip whitespace

Step 5: Handle Edge Cases
- Empty predictions: return ""
- No valid span found: return ""
- Answer at boundary: handled by window overlap

Parameters I tuned:
- max_answer_length: 30 tokens
  Initially tried 50 and 100 but got very long wrong answers
  30 tokens works best for this dataset
- Top K candidates: 20 (balance between speed and accuracy)
- Null score threshold: 0.0 (not used in final version)

This postprocessing improvement was mentioned in the slides as a TODO item.

================================================================================
8. EXPERIMENTAL CONFIGURATIONS
================================================================================

I trained four different configurations to understand how various techniques
affect performance.

--------------------------------------------------------------------------------
Configuration 1: SIMPLE
--------------------------------------------------------------------------------
Purpose: Baseline model with minimal complexity

Hyperparameters:
- Model: bert-base-chinese
- Epochs: 3
- Batch size: 16
- Learning rate: 3e-5
- Max length: 512
- Doc stride: 150
- Max answer length: 30
- Warmup ratio: 0.1
- Weight decay: 0.01
- Dropout: 0.1
- Mixed precision: No
- Gradient accumulation: 1 (disabled)

Training time: ~45 minutes
GPU memory: ~8GB

Notes:
- Quick to train, good for debugging
- No advanced techniques
- Serves as baseline for comparison

--------------------------------------------------------------------------------
Configuration 2: MEDIUM
--------------------------------------------------------------------------------
Purpose: Add learning rate scheduling and longer training

Hyperparameters:
- Model: bert-base-chinese (same as simple)
- Epochs: 5 (increased)
- Batch size: 16
- Learning rate: 3e-5
- Learning rate scheduler: Linear with warmup (ADDED)
- Max length: 512
- Doc stride: 150
- Max answer length: 30
- Warmup ratio: 0.1
- Weight decay: 0.01
- Dropout: 0.1
- Mixed precision: No
- Gradient accumulation: 2 (ADDED)

Training time: ~1.5 hours
GPU memory: ~8GB

Improvements over Simple:
- Longer training (5 vs 3 epochs)
- Learning rate scheduling for better convergence
- Gradient accumulation (effective batch size = 32)

--------------------------------------------------------------------------------
Configuration 3: STRONG
--------------------------------------------------------------------------------
Purpose: Use better model and optimization techniques

Hyperparameters:
- Model: hfl/chinese-electra-180g-base-discriminator (UPGRADED)
- Epochs: 8
- Batch size: 16
- Learning rate: 3e-5
- Learning rate scheduler: Linear with warmup
- Max length: 512
- Doc stride: 150
- Max answer length: 30 (tuned from 50)
- Warmup ratio: 0.15
- Weight decay: 0.01
- Dropout: 0.2 (increased regularization)
- Mixed precision: Yes (FP16) (ADDED)
- Gradient accumulation: 4
- Max gradient norm: 1.0

Training time: ~3 hours
GPU memory: ~10GB (with FP16)

Major improvements:
- ELECTRA model (better than BERT)
- Mixed precision training (faster, less memory)
- More epochs for convergence
- Higher dropout to prevent overfitting
- Larger effective batch size (16 Ã— 4 = 64)

--------------------------------------------------------------------------------
Configuration 4: BOSS
--------------------------------------------------------------------------------
Purpose: Ensemble of models for maximum performance

Hyperparameters:
- Model: hfl/chinese-electra-180g-base-discriminator
- Ensemble: 3 models with different random seeds
- Epochs: 15 (per model)
- Batch size: 8 (smaller to fit 3 models)
- Eval batch size: 16
- Learning rate: 5e-6 (lower for stability)
- Learning rate scheduler: Linear with warmup
- Max length: 512
- Doc stride: 150
- Max answer length: 30 (FIXED from 100)
- Warmup ratio: 0.2
- Weight decay: 0.005
- Dropout: 0.3 (high regularization)
- Mixed precision: Yes (FP16)
- Gradient accumulation: 8 (effective batch size = 64)
- Max gradient norm: 0.3 (conservative)

Training time: ~12 hours (3 models Ã— 4 hours each)
GPU memory: ~12GB per model

Ensemble strategy:
- Train 3 separate models with different random seeds
- During inference, run all 3 models
- Average the start_logits and end_logits
- Extract answer from averaged predictions

This diversifies predictions and reduces variance.

Key challenge faced:
Initially set max_answer_length to 100, which caused models to extract
very long spans (100+ characters) including questions and irrelevant context.
Fixed by reducing to 30 tokens, which matches actual answer lengths.

================================================================================
9. EVALUATION RESULTS
================================================================================

All models were evaluated on the test set (5,351 questions).

Metrics Used:
1. F1 Score: Measures token overlap between prediction and ground truth
   - Accounts for partial matches
   - More forgiving than exact match
   
2. Exact Match: Requires perfect match after normalization
   - Stricter metric
   - More realistic for real applications

Results Summary:
--------------------------------------------------------------------------------
Model       | F1 Score | Exact Match | Training Time | Empty Predictions
--------------------------------------------------------------------------------
Simple      | 46.04%   | 32.20%      | ~45 min       | 14 (0.26%)
Medium      | 46.02%   | 32.03%      | ~1.5 hours    | 12 (0.22%)
Strong      | 53.80%   | 38.70%      | ~3 hours      | 0 (0%)
Boss        | 50.96%   | 35.08%      | ~12 hours     | 0 (0%)
--------------------------------------------------------------------------------

Detailed Analysis:

SIMPLE Configuration:
- F1: 46.04%, EM: 32.20%
- Good baseline performance
- Some empty predictions (14 cases)
- Fast to train
- Limited by short training (3 epochs) and basic setup

MEDIUM Configuration:
- F1: 46.02%, EM: 32.03%
- Surprisingly similar to Simple
- Slight decrease possibly due to:
  * Same base model (BERT)
  * Gradient accumulation might need tuning
  * 5 epochs might not be enough to see benefit
- Fewer empty predictions (12 vs 14)
- Shows that just training longer doesn't always help

STRONG Configuration:
- F1: 53.80%, EM: 38.70%
- Best performing single model!
- Significant improvement over Simple/Medium:
  * +7.76% F1 improvement
  * +6.50% EM improvement
- Zero empty predictions
- Benefits from:
  * ELECTRA model (better pretraining)
  * Mixed precision training
  * Longer training (8 epochs)
  * Better regularization

BOSS Configuration (Ensemble):
- F1: 50.96%, EM: 35.08%
- Surprisingly worse than Strong!
- Expected to be best but underperformed
- Possible reasons:
  * Ensemble averaging might smooth out confident predictions
  * Lower learning rate (5e-6 vs 3e-5)
  * Very high dropout (0.3) might be too aggressive
  * Models might be too similar (same architecture)
  * 3 models might not be enough diversity
  
Unexpected finding:
Usually ensembles outperform single models, but here the Strong single model
won. This taught me that ensemble isn't always better - it depends on:
- Model diversity
- Proper hyperparameter tuning for ensemble setting
- Whether individual models are already well-optimized

Prediction Quality Analysis:
- Average answer length: 19.9 to 21.2 characters (reasonable)
- Empty predictions mostly in Simple/Medium (they're less confident)
- Strong and Boss have zero empty predictions (more confident)
- After fixing max_answer_length, no overly long predictions

================================================================================
10. ANALYSIS AND OBSERVATIONS
================================================================================

Key Findings:

1. Model Choice Matters Most
   - ELECTRA significantly outperforms BERT (+7.8% F1)
   - Better pretraining strategy pays off
   - Worth the slightly longer training time

2. Mixed Precision Training is Essential
   - Enables training with larger effective batch sizes
   - Faster training without accuracy loss
   - Crucial for resource-constrained environments

3. Preprocessing is Critical
   - Sliding windows with doc_stride=150 handles long contexts well
   - Answer centering during training helps learning
   - Proper tokenization is crucial for Chinese text

4. Postprocessing Validation Prevents Errors
   - Checking end_idx >= start_idx is essential
   - max_answer_length must match data characteristics
   - Setting it too high (100) caused major issues

5. Ensemble Doesn't Always Win
   - My Strong single model beat the Boss ensemble
   - Possible that ensemble hyperparameters need more tuning
   - Or individual models need more diversity

6. Gradient Accumulation is Useful
   - Allows simulating larger batch sizes
   - Important when GPU memory is limited
   - Must be careful with gradient clipping timing

7. Learning Rate Scheduling Helps
   - Warmup prevents early training instability
   - Decay helps fine-tune in later epochs
   - But didn't show dramatic improvement in my Medium config

What Worked Well:
- ELECTRA model
- Mixed precision training (FP16)
- Sliding windows with 150 token stride
- Validation of answer spans in postprocessing
- Setting max_answer_length = 30 tokens

What Didn't Work as Expected:
- Ensemble (Boss) underperformed single Strong model
- Medium config barely improved over Simple
- Very high dropout (0.3) might be too much
- Very low learning rate (5e-6) might be too conservative

Comparison to Literature:
- SQuAD v1.1 human performance: ~90% F1, ~80% EM
- My best model: 53.80% F1, 38.70% EM
- Gap is expected because:
  * Spoken language is noisier than written text
  * Chinese QA is harder than English
  * Using base models, not large models
  * Limited training data and epochs

================================================================================
11. CHALLENGES FACED
================================================================================

During this project, I encountered several challenges:

1. Dataset Format Mismatch
Problem: Code expected flat list, but data was in nested SQuAD format
Solution: Implemented _flatten_squad_data() to convert nested structure
Lesson: Always check data format before processing

2. Gradient Clipping with Mixed Precision
Problem: Error "unscale_() has already been called on this optimizer"
Cause: Gradient clipping called every step, but should only clip when syncing
Solution: Added condition: if accelerator.sync_gradients: clip_grad_norm_()
Lesson: Mixed precision training requires careful gradient handling

3. Model Loading Issues
Problem: Saved config was training config, not model config
Solution: Implemented fallback loading that uses base model + load weights
Used safetensors format for better compatibility
Lesson: Save both training and model configs separately

4. Ensemble Model Loading
Problem: Code expected single model, but Boss saves 3 separate models
Solution: Implemented SimpleEnsemble wrapper class that:
- Loads model_0, model_1, model_2
- Averages predictions from all models
- Returns compatible output format
Lesson: Ensemble models need special handling

5. Overly Long Predictions
Problem: Boss model predicted 100+ character answers
Cause: max_answer_length=100 tokens allowed very long spans
Solution: Reduced to 30 tokens to match actual answer lengths
Lesson: Hyperparameters must match data characteristics

6. End Index Before Start Index
Problem: Sometimes model predicted end_idx < start_idx
Cause: Model outputs independent predictions for start and end
Solution: Added validation in postprocessing to skip invalid spans
Lesson: Always validate model outputs before using them

7. Environment Setup
Problem: PyTorch CUDA version mismatch
Solution: Created environment.yml with specific CUDA version
Used conda to manage dependencies
Lesson: Reproducible environments are crucial

8. Training Time Management
Problem: Boss ensemble takes 12 hours to train
Solution: Used mixed precision and efficient batch sizes
Ran training overnight
Lesson: Plan training time for complex models

9. Weights & Biases Integration
Problem: Needed to log in and set up project
Solution: Followed wandb setup guide, created project
Lesson: Experiment tracking is valuable for comparing runs

10. Chinese Text Processing
Problem: Chinese tokenization is different from English
Solution: Used pretrained Chinese tokenizers (BERT/ELECTRA)
Trust the library's tokenization
Lesson: Use language-specific tools for non-English NLP

================================================================================
12. CONCLUSION
================================================================================

This project successfully implemented an extractive Question Answering system
for the Spoken-SQuAD dataset with the following achievements:

Final Results:
- Best model: Strong configuration (ELECTRA-base)
- F1 Score: 53.80%
- Exact Match: 38.70%
- Zero empty predictions
- Handles long contexts with sliding windows

Technical Skills Gained:
1. Transformer-based QA model implementation
2. Mixed precision training with Accelerate
3. Ensemble model training and inference
4. Advanced preprocessing (sliding windows, answer centering)
5. Robust postprocessing with validation
6. Experiment tracking with Weights & Biases
7. Conda environment management
8. Working with Chinese NLP models

Key Takeaways:
- Model architecture choice is crucial (ELECTRA > BERT)
- Preprocessing and postprocessing are as important as model training
- Bigger/more complex doesn't always mean better (Strong > Boss)
- Hyperparameter tuning requires understanding the data
- Validation and error handling prevent bugs in production

Improvements for Future Work:
1. Try larger models (ELECTRA-large, RoBERTa-large)
2. Fine-tune ensemble hyperparameters better
3. Experiment with different ensemble strategies (voting, stacking)
4. Try data augmentation techniques
5. Implement more sophisticated answer extraction (beam search)
6. Use knowledge distillation to create efficient models
7. Experiment with recent models like DeBERTa or ALBERT
8. Try different answer length limits for different question types

What I Learned:
The most important lesson from this project is that good deep learning is not
just about using the fanciest model. It's about understanding your data,
implementing robust preprocessing and postprocessing, validating outputs, and
systematically experimenting with different approaches. The debugging process
taught me as much as the successful results.

The fact that my Strong single model outperformed the Boss ensemble was a
valuable lesson - sometimes simpler, well-tuned models beat complex ensembles.
This emphasizes the importance of proper hyperparameter tuning and understanding
when complexity helps vs. when it hurts.

Overall, this project provided hands-on experience with modern NLP techniques
and taught me how to build production-ready QA systems.

================================================================================
END OF REPORT
================================================================================
